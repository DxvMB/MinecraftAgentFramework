from transformers import AutoTokenizer, AutoModelForCausalLM
from MinecraftAgentFramework.mcpi.minecraft import Minecraft
import time
import torch


class ChatBotAgent:
    def __init__(self, model_name="facebook/opt-350m"):
        """
        Initialize the chatbot with the Facebook OPT model for conversational interaction.
        Connect to Minecraft for reading and responding to chat messages.
        """
        print(f"Loading model {model_name}...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.mc = Minecraft.create()  # Connect to the Minecraft world
        self.running = True
        print(f"Model {model_name} loaded successfully.")
        print("Connected to Minecraft.")

    def query_llm(self, prompt, max_length=150):
        """
        Generate a response to the input prompt using the model.
        Args:
            prompt (str): The input prompt to pass to the model.
            max_length (int): The maximum length for the generated response.

        Returns:
            str: Response text generated by the model.
        """
        try:
            # Dynamically detect device (GPU or CPU)
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            self.model.to(device)  # Ensure the model is on the correct device

            # Tokenize the input prompt and move it to the same device as the model
            inputs = self.tokenizer(prompt, return_tensors="pt", padding=True)
            inputs = {key: value.to(device) for key, value in inputs.items()}

            # Generate a response
            outputs = self.model.generate(
                inputs["input_ids"],
                attention_mask=inputs["attention_mask"],  # Explicitly set the attention mask
                max_length=max_length,
                pad_token_id=self.tokenizer.eos_token_id,  # Use EOS token for termination
                do_sample=True,  # Enable sampling
                temperature=0.7,  # Slight randomness for variety
                top_p=0.9  # Use nucleus

            )

            # Decode the response to readable text
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

            # Extract the portion after "ChatBot responds:"
            if "ChatBot responds:" in response:
                response = response.split("ChatBot responds:")[-1].strip()

            # Remove potential empty responses
            if not response.strip():
                response = "I'm not sure how to respond to that."

            return response

        except Exception as e:
            print(f"Error querying the model: {e}")
            return "Sorry, I couldn't process your message."

    def read_minecraft_chat(self):
        """
        Poll Minecraft chat messages. Returns the latest message if available, else None.

        Returns:
            tuple: (message, sender), where sender is the entity ID of the player, and
            message is the text of the chat message.
        """
        chat_events = list(self.mc.events.pollChatPosts())
        if not chat_events:
            return None, None

        message = chat_events[-1].message  # Most recent message
        sender = chat_events[-1].entityId
        return message, sender

    def send_minecraft_message(self, message):
        """
        Send a message to Minecraft chat.

        Args:
            message (str): Text to send to Minecraft.
        """
        self.mc.postToChat(message)

    def run(self):
        """
            Main loop for managing chatbot responses in Minecraft chat.
            Reads messages from Minecraft, processes them with the model, and responds back.
            """
        print("ChatBotAgent is running...")
        while self.running:
            try:
                # Read messages from Minecraft chat
                message, sender = self.read_minecraft_chat()

                if message:
                    print(f"Received message: {message}")

                    # Add a context prefix to the prompt (improve formatting)
                    prompt = (
                        f"The following is a conversation between a player and a chatbot in Minecraft.\n"
                        f"Player said: {message}\nChatBot responds:"
                    )

                    # Query the chatbot model to generate a response
                    bot_response = self.query_llm(prompt)

                    # Provide a fallback message if the response is empty
                    if not bot_response.strip():
                        bot_response = "I'm not sure how to respond to that."

                    # Post the response to Minecraft
                    self.send_minecraft_message(bot_response)
                    print(f"Sent response: {bot_response}")

                # Delay to avoid excessive polling
                time.sleep(1)

            except KeyboardInterrupt:
                print("Exiting ChatBotAgent...")
                self.running = False  # Stop the loop on manual interruption
            except Exception as e:
                print(f"An error occurred: {e}")

